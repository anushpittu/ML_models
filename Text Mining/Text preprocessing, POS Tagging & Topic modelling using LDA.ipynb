{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MINI PROJECT 1  - The TF-IDF Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "import math\n",
    "import itertools\n",
    "import unicodedata\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "from __future__ import division\n",
    "from pprint import pprint\n",
    "from nltk.tokenize import  word_tokenize \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_file = open('TF-IDF_dataset.txt','r',encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpus_file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \" \".join([word for word in corpus.split() if word not in STOPWORDS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Remove the punctuations. the special characters and convert the text to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ''.join(e for e in corpus if (e.isalnum() or e.isspace()))\n",
    "corpus = ''.join([c for c in corpus if c not in string.punctuation])\n",
    "corpus = corpus.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus\n",
    "## uncomment the above line of code to print out the enitre corpus in a single sring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('TF-IDF_dataset.txt','r',encoding='utf8') as file:\n",
    "    all_chapters = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter_dict = {}\n",
    "\n",
    "indexes = []\n",
    "for i in range(1,9):\n",
    "    indexes.append(f\"Chapter {i}\")\n",
    "\n",
    "for i in range(0,8):\n",
    "    if i == 7:\n",
    "        str1 = indexes[i]\n",
    "        start_chapter = all_chapters.find(str1)\n",
    "        chapter_dict[\"chapter{0}\".format(i+1)] = all_chapters[start_chapter+9:]\n",
    "\n",
    "    else:\n",
    "        str1 = indexes[i]\n",
    "        str2 = indexes[i+1]\n",
    "        start_chapter = all_chapters.find(str1)\n",
    "        end_chapter = all_chapters.find(str2)\n",
    "        if start_chapter == -1:\n",
    "            start_chapter = all_chapters.find(str1.lower())\n",
    "        if end_chapter == -1:\n",
    "            end_chapter = all_chapters.find(str2.lower())\n",
    "\n",
    "        chapter_dict[\"chapter{0}\".format(i+1)] = all_chapters[start_chapter+9:end_chapter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,8):\n",
    "    this_key = list(chapter_dict.keys())[i]\n",
    "    current_str = chapter_dict.get(this_key)\n",
    "    current_str = \" \".join([word for word in str(current_str).split() if word not in STOPWORDS])\n",
    "    chapter_dict[this_key] = current_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,8):\n",
    "    this_key = list(chapter_dict.keys())[i]\n",
    "    current_str = chapter_dict.get(this_key)\n",
    "    current_str = ''.join(e for e in current_str if (e.isalnum() or e.isspace()))\n",
    "    current_str = ''.join([c for c in current_str if c not in string.punctuation])\n",
    "    chapter_dict[this_key] = current_str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create bigrams and trigrams for the entire dataset and list down 20 most frequent bigram and 20 most frequent trigrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_word_list = corpus.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(i, found)               2\n",
       "(natural, philosophy)    2\n",
       "(ingolstadt, i)          2\n",
       "(when, i)                2\n",
       "(it, already)            2\n",
       "(me, i)                  2\n",
       "(thought, returning)     2\n",
       "(m, waldman)             2\n",
       "(return, us)             2\n",
       "(my, father)             2\n",
       "(long, time)             2\n",
       "(native, country)        2\n",
       "(two, years)             2\n",
       "(i, thought)             2\n",
       "(her, she)               2\n",
       "(i, made)                2\n",
       "(cause, death)           1\n",
       "(mockery, justice)       1\n",
       "(rapid, my)              1\n",
       "(rowing, lake)           1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pd.Series(nltk.ngrams(corpus_word_list, 2)).value_counts())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(mountains, changes, seasons)       1\n",
       "(cruel, kindness, i)                1\n",
       "(clerval, son, merchant)            1\n",
       "(cannot, rendered, callous)         1\n",
       "(syndics, father, filled)           1\n",
       "(rapidly, end, two)                 1\n",
       "(roncesvalles, round, table)        1\n",
       "(stars, often, disappeared)         1\n",
       "(clear, facile, apprehension)       1\n",
       "(father, rest, family)              1\n",
       "(unfortunate, circumstances, he)    1\n",
       "(shapes, mountains, changes)        1\n",
       "(league, city, we)                  1\n",
       "(eye, creature, open)               1\n",
       "(friendship, one, among)            1\n",
       "(eye, saw, us)                      1\n",
       "(chapter, 3, when)                  1\n",
       "(knightly, adventure, he)           1\n",
       "(eye, skims, page)                  1\n",
       "(persuade, mother, refrain)         1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pd.Series(nltk.ngrams(corpus_word_list, 3)).value_counts())[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. You have to implement TF-IDF the Algorithm from scratch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, bow):\n",
    "    tfDict = {}\n",
    "    bowCount = len(bow)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(bowCount)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for doc in docList:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        if val != 0:\n",
    "            idfDict[word] = math.log10(N / float(val))\n",
    "        else:\n",
    "            idfDict[word] = math.log10(N / float(val+1))\n",
    "        \n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Use the above-implemented algorithm and the values to calculate TF-IDF (using TF IDF formula) on the dataset and list down the top 10 words which have the highest TF-IDF Value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set = [ ]\n",
    "x = corpus.split(\" \")\n",
    "for word in x:\n",
    "    if word not in word_set:\n",
    "        word_set.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow1 = chapter_dict.get('chapter1').split(\" \")\n",
    "bow2 = chapter_dict.get('chapter2').split(\" \")\n",
    "bow3 = chapter_dict.get('chapter3').split(\" \")\n",
    "bow4 = chapter_dict.get('chapter4').split(\" \")\n",
    "bow5 = chapter_dict.get('chapter5').split(\" \")\n",
    "bow6 = chapter_dict.get('chapter6').split(\" \")\n",
    "bow7 = chapter_dict.get('chapter7').split(\" \")\n",
    "bow8 = chapter_dict.get('chapter8').split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDict1 = dict.fromkeys(word_set, 0) \n",
    "wordDict2 = dict.fromkeys(word_set, 0)\n",
    "wordDict3 = dict.fromkeys(word_set, 0) \n",
    "wordDict4 = dict.fromkeys(word_set, 0)\n",
    "wordDict5 = dict.fromkeys(word_set, 0) \n",
    "wordDict6 = dict.fromkeys(word_set, 0)\n",
    "wordDict7 = dict.fromkeys(word_set, 0) \n",
    "wordDict8 = dict.fromkeys(word_set, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in bow1:\n",
    "    wordDict1[word]+=1\n",
    "    \n",
    "for word in bow2:\n",
    "    wordDict2[word]+=1\n",
    "    \n",
    "for word in bow3:\n",
    "    wordDict3[word]+=1\n",
    "    \n",
    "for word in bow4:\n",
    "    wordDict4[word]+=1\n",
    "\n",
    "for word in bow5:\n",
    "    wordDict5[word]+=1\n",
    "    \n",
    "for word in bow6:\n",
    "    wordDict6[word]+=1\n",
    "\n",
    "for word in bow7:\n",
    "    wordDict7[word]+=1\n",
    "\n",
    "for word in bow8:\n",
    "    wordDict8[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chapter</th>\n",
       "      <th>1</th>\n",
       "      <th>i</th>\n",
       "      <th>birth</th>\n",
       "      <th>genevese</th>\n",
       "      <th>family</th>\n",
       "      <th>one</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>republic</th>\n",
       "      <th>my</th>\n",
       "      <th>...</th>\n",
       "      <th>seated</th>\n",
       "      <th>tear</th>\n",
       "      <th>dim</th>\n",
       "      <th>recovered</th>\n",
       "      <th>herself</th>\n",
       "      <th>look</th>\n",
       "      <th>sorrowful</th>\n",
       "      <th>attest</th>\n",
       "      <th>utter</th>\n",
       "      <th>guiltlessness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 945 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   chapter  1   i  birth  genevese  family  one  distinguished  republic  my  \\\n",
       "0        0  0   2      1         1       2    2              2         1   2   \n",
       "1        0  0   7      1         0       0    1              0         0   0   \n",
       "2        0  0   8      0         0       0    0              0         0   3   \n",
       "3        0  0  13      0         0       0    2              0         0   2   \n",
       "4        0  0   7      0         0       0    1              0         0   0   \n",
       "5        0  0   6      0         0       0    1              0         0   1   \n",
       "6        0  0   8      0         0       0    0              0         0   0   \n",
       "7        0  0   5      0         0       1    1              0         0   1   \n",
       "\n",
       "   ...  seated  tear  dim  recovered  herself  look  sorrowful  attest  utter  \\\n",
       "0  ...       0     0    0          0        0     0          0       0      0   \n",
       "1  ...       0     0    0          0        0     0          0       0      0   \n",
       "2  ...       0     0    0          0        0     0          0       0      0   \n",
       "3  ...       0     0    0          0        0     0          0       0      0   \n",
       "4  ...       0     0    0          0        0     0          0       0      0   \n",
       "5  ...       0     0    0          0        0     0          0       0      0   \n",
       "6  ...       0     0    0          0        0     0          0       0      0   \n",
       "7  ...       1     1    1          1        1     1          1       1      1   \n",
       "\n",
       "   guiltlessness  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  \n",
       "5              0  \n",
       "6              0  \n",
       "7              1  \n",
       "\n",
       "[8 rows x 945 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([wordDict1, wordDict2, wordDict3, wordDict4, wordDict5, wordDict6, wordDict7, wordDict8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfBow1 = computeTF(wordDict1, bow1)\n",
    "tfBow2 = computeTF(wordDict2, bow2)\n",
    "tfBow3 = computeTF(wordDict3, bow3)\n",
    "tfBow4 = computeTF(wordDict4, bow4)\n",
    "tfBow5 = computeTF(wordDict5, bow5)\n",
    "tfBow6 = computeTF(wordDict6, bow6)\n",
    "tfBow7 = computeTF(wordDict7, bow7)\n",
    "tfBow8 = computeTF(wordDict8, bow8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs = computeIDF([wordDict1, wordDict2, wordDict3, wordDict4, wordDict5, wordDict6, wordDict7, wordDict8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfBow1 = computeTFIDF(tfBow1, idfs)\n",
    "tfidfBow2 = computeTFIDF(tfBow2, idfs)\n",
    "tfidfBow3 = computeTFIDF(tfBow3, idfs)\n",
    "tfidfBow4 = computeTFIDF(tfBow4, idfs)\n",
    "tfidfBow5 = computeTFIDF(tfBow5, idfs)\n",
    "tfidfBow6 = computeTFIDF(tfBow6, idfs)\n",
    "tfidfBow7 = computeTFIDF(tfBow7, idfs)\n",
    "tfidfBow8 = computeTFIDF(tfBow8, idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame([tfidfBow1, tfidfBow2, tfidfBow3, tfidfBow4, tfidfBow5, tfidfBow6, tfidfBow7, tfidfBow8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 945)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "he               0.003679\n",
       "would            0.002654\n",
       "circumstances    0.002509\n",
       "return           0.002136\n",
       "father           0.002125\n",
       "yellow           0.002016\n",
       "limbs            0.002016\n",
       "almost           0.002016\n",
       "black            0.002016\n",
       "you              0.002008\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.mean(axis = 0).sort_values(ascending = False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Label the cleaned Tf-IDF dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tagged_list = nltk.pos_tag(corpus.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1381"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Tagged_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('chapter', 'NN'), ('1', 'CD'), ('i', 'JJ'), ('birth', 'NN'), ('genevese', 'JJ'), ('family', 'NN'), ('one', 'CD'), ('distinguished', 'VBN'), ('republic', 'JJ'), ('my', 'PRP$'), ('ancestors', 'NNS'), ('many', 'JJ'), ('years', 'NNS'), ('counsellors', 'NNS'), ('syndics', 'VBP'), ('father', 'RB'), ('filled', 'VBN'), ('several', 'JJ'), ('public', 'JJ'), ('situations', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "print(Tagged_list[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Split the Train and the Test Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for i in range(0,8):\n",
    "    key = list(chapter_dict.keys())[i]\n",
    "    append_list = chapter_dict.get(key).split()\n",
    "    append_tag_list = nltk.pos_tag(append_list)\n",
    "    documents.append(append_tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the tagged sentences\n",
    "sent_tag = documents\n",
    "mod_sent_tag=[]\n",
    "for s in sent_tag:\n",
    "  s.insert(0,('##','##'))\n",
    "  s.append(('&&','&&'))\n",
    "  mod_sent_tag.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data for train and test\n",
    "split_num = int(len(mod_sent_tag)*0.9)\n",
    "train_data = mod_sent_tag[0:split_num]\n",
    "test_data = mod_sent_tag[split_num:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Implement the Viterbi Algorithm to get the Part of Speech Tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dictionary whose keys are tags and values contain words which were assigned the correspoding tag\n",
    "# ex:- 'TAG':{word1: count(word1,'TAG')}\n",
    "train_word_tag = {}\n",
    "for s in train_data:\n",
    "  for (w,t) in s:\n",
    "    w=w.lower()\n",
    "    try:\n",
    "      try:\n",
    "        train_word_tag[t][w]+=1\n",
    "      except:\n",
    "        train_word_tag[t][w]=1\n",
    "    except:\n",
    "      train_word_tag[t]={w:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the emission probabilities using train_word_tag\n",
    "train_emission_prob={}\n",
    "for k in train_word_tag.keys():\n",
    "  train_emission_prob[k]={}\n",
    "  count = sum(train_word_tag[k].values())\n",
    "  for k2 in train_word_tag[k].keys():\n",
    "    train_emission_prob[k][k2]=train_word_tag[k][k2]/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimating the bigram of tags to be used for transition probability\n",
    "bigram_tag_data = {}\n",
    "for s in train_data:\n",
    "  bi=list(nltk.bigrams(s))\n",
    "  for b1,b2 in bi:\n",
    "    try:\n",
    "      try:\n",
    "        bigram_tag_data[b1[1]][b2[1]]+=1\n",
    "      except:\n",
    "        bigram_tag_data[b1[1]][b2[1]]=1\n",
    "    except:\n",
    "      bigram_tag_data[b1[1]]={b2[1]:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the probabilities of tag bigrams for transition probability  \n",
    "bigram_tag_prob={}\n",
    "for k in bigram_tag_data.keys():\n",
    "  bigram_tag_prob[k]={}\n",
    "  count=sum(bigram_tag_data[k].values())\n",
    "  for k2 in bigram_tag_data[k].keys():\n",
    "    bigram_tag_prob[k][k2]=bigram_tag_data[k][k2]/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the possible tags for each word\n",
    "#Note: Here we have used the whole data(Train+Test)\n",
    "#Reason: There may be some words which are not present in train data but are present in test data \n",
    "tags_of_tokens = {}\n",
    "count=0\n",
    "for s in train_data:\n",
    "  for (w,t) in s:\n",
    "    w=w.lower()\n",
    "    try:\n",
    "      if t not in tags_of_tokens[w]:\n",
    "        tags_of_tokens[w].append(t)\n",
    "    except:\n",
    "      l = []\n",
    "      l.append(t)\n",
    "      tags_of_tokens[w] = l\n",
    "        \n",
    "for s in test_data:\n",
    "  for (w,t) in s:\n",
    "    w=w.lower()\n",
    "    try:\n",
    "      if t not in tags_of_tokens[w]:\n",
    "        tags_of_tokens[w].append(t)\n",
    "    except:\n",
    "      l = []\n",
    "      l.append(t)\n",
    "      tags_of_tokens[w] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividing the test data into test words and test tags\n",
    "test_words=[]\n",
    "test_tags=[]\n",
    "for s in test_data:\n",
    "  temp_word=[]\n",
    "  temp_tag=[]\n",
    "  for (w,t) in s:\n",
    "    temp_word.append(w.lower())\n",
    "    temp_tag.append(t)\n",
    "  test_words.append(temp_word)\n",
    "  test_tags.append(temp_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Executing the Viterbi Algorithm\n",
    "predicted_tags = []                #intializing the predicted tags\n",
    "for x in range(len(test_words)):   # for each tokenized sentence in the test data\n",
    "  s = test_words[x]\n",
    "  #storing_values is a dictionary which stores the required values\n",
    "  #ex: storing_values = {step_no.:{state1:[previous_best_state,value_of_the_state]}}                \n",
    "  storing_values = {}              \n",
    "  for q in range(len(s)):\n",
    "    step = s[q]\n",
    "    #for the starting word of the sentence\n",
    "    if q == 1:                \n",
    "      storing_values[q] = {}\n",
    "      tags = tags_of_tokens[step]\n",
    "      for t in tags:\n",
    "        #this is applied since we do not know whether the word in the test data is present in train data or not\n",
    "        try:\n",
    "          storing_values[q][t] = ['##',bigram_tag_prob['##'][t]*train_emission_prob[t][step]]\n",
    "        #if word is not present in the train data but present in test data we assign a very low probability of 0.0001\n",
    "        except:\n",
    "          storing_values[q][t] = ['##',0.0001]#*train_emission_prob[t][step]]\n",
    "    \n",
    "    #if the word is not at the start of the sentence\n",
    "    if q>1:\n",
    "      storing_values[q] = {}\n",
    "      previous_states = list(storing_values[q-1].keys())   # loading the previous states\n",
    "      current_states  = tags_of_tokens[step]               # loading the current states\n",
    "      #calculation of the best previous state for each current state and then storing\n",
    "      #it in storing_values\n",
    "      for t in current_states:                             \n",
    "        temp = []\n",
    "        for pt in previous_states:                         \n",
    "          try:\n",
    "            temp.append(storing_values[q-1][pt][1]*bigram_tag_prob[pt][t]*train_emission_prob[t][step])\n",
    "          except:\n",
    "            temp.append(storing_values[q-1][pt][1]*0.0001)\n",
    "        max_temp_index = temp.index(max(temp))\n",
    "        best_pt = previous_states[max_temp_index]\n",
    "        storing_values[q][t]=[best_pt,max(temp)]\n",
    "\n",
    "  #Backtracing to extract the best possible tags for the sentence\n",
    "  pred_tags = []\n",
    "  total_steps_num = storing_values.keys()\n",
    "  last_step_num = max(total_steps_num)\n",
    "  for bs in range(len(total_steps_num)):\n",
    "    step_num = last_step_num - bs\n",
    "    if step_num == last_step_num:\n",
    "      pred_tags.append('&&')\n",
    "      pred_tags.append(storing_values[step_num]['&&'][0])\n",
    "    if step_num<last_step_num and step_num>0:\n",
    "      pred_tags.append(storing_values[step_num][pred_tags[len(pred_tags)-1]][0])\n",
    "  predicted_tags.append(list(reversed(pred_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the accuracy based on tagging each word in the test data.\n",
    "right = 0 \n",
    "wrong = 0\n",
    "for i in range(len(test_tags)):\n",
    "  gt = test_tags[i]\n",
    "  pred = predicted_tags[i]\n",
    "  for h in range(len(gt)):\n",
    "    if gt[h] == pred[h]:\n",
    "      right = right+1\n",
    "    else:\n",
    "      wrong = wrong +1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Calculate the Accuracy and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_set = set(test_words[0])\n",
    "unique_list = (list(list_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy calculated on the test data is:  0.953757225433526\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy calculated on the test data is: ',right/(right+wrong))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Precision = right/(right+wrong)\n",
    "recall    = right/len(unique_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_score = (2*Precision*recall)/(Precision+recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### This above formula is referenced from link: https://www.cl.cam.ac.uk/teaching/1920/MLRD/tasks/task8_copy.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_score calculated on the test data is:  1.015384615384615\n"
     ]
    }
   ],
   "source": [
    "print('F1_score calculated on the test data is: ',F1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### F1-score calculated from the formula given along with the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on the test data is:  1.0855263157894737\n"
     ]
    }
   ],
   "source": [
    "print('F1-score on the test data is: ',right/len(unique_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Score calculated for each class from the sklearn's classification report metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = []\n",
    "y_pred = []\n",
    "for i in range(1,len(predicted_tags[0])-1):\n",
    "    y_pred.append(predicted_tags[0][i])\n",
    "    y_test.append(test_tags[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       1.00      1.00      1.00         1\n",
      "          CD       1.00      1.00      1.00         3\n",
      "          DT       1.00      1.00      1.00         4\n",
      "          IN       1.00      1.00      1.00         3\n",
      "          JJ       0.92      0.85      0.88        26\n",
      "          MD       1.00      1.00      1.00         6\n",
      "          NN       0.96      0.96      0.96        50\n",
      "         NNS       1.00      1.00      1.00        12\n",
      "         PRP       1.00      1.00      1.00         7\n",
      "        PRP$       1.00      1.00      1.00         1\n",
      "          RB       1.00      1.00      1.00        13\n",
      "          VB       1.00      1.00      1.00         5\n",
      "         VBD       0.89      0.94      0.92        18\n",
      "         VBG       1.00      1.00      1.00         3\n",
      "         VBN       0.93      0.93      0.93        14\n",
      "         VBP       0.80      1.00      0.89         4\n",
      "         WRB       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.95       171\n",
      "   macro avg       0.97      0.98      0.98       171\n",
      "weighted avg       0.95      0.95      0.95       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Using the LDA algorithm create the Topics (10) for the Corpus  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_list = list(chapter_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for i in range(0,8):\n",
    "    documents.append(chapter_dict.get(documents_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = [sent.split() for sent in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(data_words)\n",
    "texts = data_words\n",
    "whole_file = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of topics\n",
    "num_topics = 10\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=whole_file,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics,\n",
    "                                      passes = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. List down the 10 words in each of the Topics Extracted.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.001*\"infinite\" + 0.001*\"how\" + 0.001*\"limbs\" + 0.001*\"lifeless\" + '\n",
      "  '0.001*\"lay\" + 0.001*\"infuse\" + 0.001*\"lustrous\" + 0.001*\"god\" + '\n",
      "  '0.001*\"halfextinguished\" + 0.001*\"might\"'),\n",
      " (1,\n",
      "  '0.001*\"infinite\" + 0.001*\"how\" + 0.001*\"limbs\" + 0.001*\"lifeless\" + '\n",
      "  '0.001*\"lay\" + 0.001*\"infuse\" + 0.001*\"lustrous\" + 0.001*\"god\" + '\n",
      "  '0.001*\"halfextinguished\" + 0.001*\"might\"'),\n",
      " (2,\n",
      "  '0.032*\"i\" + 0.014*\"you\" + 0.009*\"my\" + 0.007*\"could\" + 0.007*\"yet\" + '\n",
      "  '0.007*\"she\" + 0.007*\"elizabeth\" + 0.007*\"upon\" + 0.007*\"he\" + 0.005*\"time\"'),\n",
      " (3,\n",
      "  '0.019*\"i\" + 0.015*\"would\" + 0.012*\"justine\" + 0.008*\"seemed\" + 0.008*\"she\" '\n",
      "  '+ 0.008*\"a\" + 0.008*\"yet\" + 0.008*\"suffered\" + 0.008*\"cause\" + '\n",
      "  '0.008*\"committed\"'),\n",
      " (4,\n",
      "  '0.001*\"infinite\" + 0.001*\"how\" + 0.001*\"limbs\" + 0.001*\"lifeless\" + '\n",
      "  '0.001*\"lay\" + 0.001*\"infuse\" + 0.001*\"lustrous\" + 0.001*\"god\" + '\n",
      "  '0.001*\"halfextinguished\" + 0.001*\"might\"'),\n",
      " (5,\n",
      "  '0.048*\"i\" + 0.007*\"it\" + 0.007*\"great\" + 0.005*\"in\" + 0.005*\"pursuit\" + '\n",
      "  '0.005*\"found\" + 0.005*\"m\" + 0.005*\"two\" + 0.005*\"one\" + 0.005*\"return\"'),\n",
      " (6,\n",
      "  '0.020*\"he\" + 0.020*\"i\" + 0.007*\"life\" + 0.007*\"we\" + 0.007*\"father\" + '\n",
      "  '0.007*\"one\" + 0.007*\"country\" + 0.007*\"circumstances\" + 0.007*\"deeply\" + '\n",
      "  '0.007*\"them\"'),\n",
      " (7,\n",
      "  '0.001*\"infinite\" + 0.001*\"how\" + 0.001*\"limbs\" + 0.001*\"lifeless\" + '\n",
      "  '0.001*\"lay\" + 0.001*\"infuse\" + 0.001*\"lustrous\" + 0.001*\"god\" + '\n",
      "  '0.001*\"halfextinguished\" + 0.001*\"might\"'),\n",
      " (8,\n",
      "  '0.001*\"infinite\" + 0.001*\"how\" + 0.001*\"limbs\" + 0.001*\"lifeless\" + '\n",
      "  '0.001*\"lay\" + 0.001*\"infuse\" + 0.001*\"lustrous\" + 0.001*\"god\" + '\n",
      "  '0.001*\"halfextinguished\" + 0.001*\"might\"'),\n",
      " (9,\n",
      "  '0.001*\"infinite\" + 0.001*\"how\" + 0.001*\"limbs\" + 0.001*\"lifeless\" + '\n",
      "  '0.001*\"lay\" + 0.001*\"infuse\" + 0.001*\"lustrous\" + 0.001*\"god\" + '\n",
      "  '0.001*\"halfextinguished\" + 0.001*\"might\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el187619023730194247185433141\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el187619023730194247185433141_data = {\"mdsDat\": {\"x\": [-0.12026697635658812, 0.12189505927231398, 0.10701086318246432, -0.13179536387781218, 0.0038552556495829895, 0.003861209923156361, 0.003865278515295395, 0.0038540262813798066, 0.0038618447541493408, 0.003858802656058174], \"y\": [0.1198349390167314, -0.1382203383187937, 0.14835564565465953, -0.10535366841045857, -0.004103259444205637, -0.00410223195043313, -0.004102080206486339, -0.004103536543925884, -0.004102421428332115, -0.0041030483687555545], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [35.810638492535915, 26.0664322229066, 25.2605496312351, 12.51288559352091, 0.05824900996691131, 0.05824900996691131, 0.05824900996691131, 0.05824900996691131, 0.05824900996691131, 0.05824900996691131]}, \"tinfo\": {\"Term\": [\"he\", \"you\", \"would\", \"great\", \"it\", \"m\", \"pursuit\", \"in\", \"his\", \"found\", \"country\", \"return\", \"yet\", \"son\", \"years\", \"two\", \"first\", \"thought\", \"day\", \"long\", \"ernest\", \"she\", \"elizabeth\", \"father\", \"them\", \"deeply\", \"circumstances\", \"we\", \"upon\", \"my\", \"great\", \"in\", \"m\", \"pursuit\", \"his\", \"closely\", \"discoveries\", \"engaged\", \"gone\", \"instruments\", \"know\", \"krempe\", \"light\", \"may\", \"morning\", \"natural\", \"nearly\", \"philosophy\", \"proficiency\", \"progress\", \"returning\", \"science\", \"sense\", \"sought\", \"waldman\", \"went\", \"whilst\", \"almost\", \"already\", \"black\", \"i\", \"it\", \"two\", \"return\", \"found\", \"one\", \"you\", \"became\", \"ardour\", \"made\", \"university\", \"often\", \"study\", \"true\", \"years\", \"even\", \"me\", \"when\", \"circumstances\", \"deeply\", \"them\", \"beaufort\", \"birth\", \"disposition\", \"distinguished\", \"friendship\", \"loved\", \"merchant\", \"poverty\", \"public\", \"united\", \"among\", \"characters\", \"together\", \"act\", \"adventure\", \"aerial\", \"ages\", \"akin\", \"alpine\", \"ample\", \"appearances\", \"arthur\", \"attach\", \"avoid\", \"began\", \"belrive\", \"blood\", \"he\", \"we\", \"family\", \"country\", \"parents\", \"many\", \"therefore\", \"world\", \"father\", \"life\", \"one\", \"the\", \"passed\", \"i\", \"geneva\", \"us\", \"son\", \"upon\", \"attending\", \"children\", \"fever\", \"future\", \"her\", \"illness\", \"looks\", \"love\", \"mother\", \"myself\", \"necessary\", \"resolved\", \"sickbed\", \"dear\", \"ill\", \"journey\", \"pleased\", \"uncle\", \"age\", \"alarming\", \"alas\", \"all\", \"another\", \"arguments\", \"attained\", \"attendants\", \"attentions\", \"been\", \"befitting\", \"elizabeth\", \"you\", \"could\", \"hope\", \"enter\", \"care\", \"attended\", \"become\", \"ingolstadt\", \"never\", \"she\", \"yet\", \"i\", \"my\", \"he\", \"time\", \"on\", \"happy\", \"country\", \"father\", \"life\", \"us\", \"day\", \"first\", \"justine\", \"appearance\", \"cause\", \"committed\", \"court\", \"innocence\", \"obliterated\", \"quickly\", \"suffered\", \"adduced\", \"aggravation\", \"also\", \"although\", \"always\", \"appeared\", \"ascribed\", \"attend\", \"attest\", \"babe\", \"beauty\", \"beings\", \"commence\", \"confessed\", \"confident\", \"confusion\", \"considered\", \"constrained\", \"courage\", \"crime\", \"decided\", \"would\", \"seemed\", \"a\", \"she\", \"yet\", \"i\", \"affection\", \"me\", \"happy\", \"eyes\", \"might\", \"saw\", \"absent\", \"discovered\", \"kindness\", \"rendered\", \"life\", \"calm\", \"make\", \"the\", \"us\", \"adduced\", \"aggravation\", \"also\", \"although\", \"always\", \"appeared\", \"ascribed\", \"attend\", \"attest\", \"babe\", \"beauty\", \"beings\", \"commence\", \"confessed\", \"confident\", \"confusion\", \"considered\", \"constrained\", \"courage\", \"crime\", \"decided\", \"declaration\", \"devices\", \"dim\", \"dreadfully\", \"dressed\", \"eleven\", \"engaging\", \"enormity\", \"entered\", \"affairs\", \"affection\", \"ancestors\", \"as\", \"assistance\", \"attention\", \"bear\", \"beaufort\", \"became\", \"begin\", \"birth\", \"bitterly\", \"business\", \"cannot\", \"character\", \"circumstances\", \"conduct\", \"could\", \"counsellors\", \"country\", \"credit\", \"daughter\", \"days\", \"debts\", \"decline\", \"deeply\", \"deplored\", \"disposition\", \"distinguished\", \"early\", \"adduced\", \"aggravation\", \"also\", \"although\", \"always\", \"appeared\", \"ascribed\", \"attend\", \"attest\", \"babe\", \"beauty\", \"beings\", \"commence\", \"confessed\", \"confident\", \"confusion\", \"considered\", \"constrained\", \"courage\", \"crime\", \"decided\", \"declaration\", \"devices\", \"dim\", \"dreadfully\", \"dressed\", \"eleven\", \"engaging\", \"enormity\", \"entered\", \"affairs\", \"affection\", \"ancestors\", \"as\", \"assistance\", \"attention\", \"bear\", \"beaufort\", \"became\", \"begin\", \"birth\", \"bitterly\", \"business\", \"cannot\", \"character\", \"circumstances\", \"conduct\", \"could\", \"counsellors\", \"country\", \"credit\", \"daughter\", \"days\", \"debts\", \"decline\", \"deeply\", \"deplored\", \"disposition\", \"distinguished\", \"early\", \"adduced\", \"aggravation\", \"also\", \"although\", \"always\", \"appeared\", \"ascribed\", \"attend\", \"attest\", \"babe\", \"beauty\", \"beings\", \"commence\", \"confessed\", \"confident\", \"confusion\", \"considered\", \"constrained\", \"courage\", \"crime\", \"decided\", \"declaration\", \"devices\", \"dim\", \"dreadfully\", \"dressed\", \"eleven\", \"engaging\", \"enormity\", \"entered\", \"affairs\", \"affection\", \"ancestors\", \"as\", \"assistance\", \"attention\", \"bear\", \"beaufort\", \"became\", \"begin\", \"birth\", \"bitterly\", \"business\", \"cannot\", \"character\", \"circumstances\", \"conduct\", \"could\", \"counsellors\", \"country\", \"credit\", \"daughter\", \"days\", \"debts\", \"decline\", \"deeply\", \"deplored\", \"disposition\", \"distinguished\", \"early\", \"adduced\", \"aggravation\", \"also\", \"although\", \"always\", \"appeared\", \"ascribed\", \"attend\", \"attest\", \"babe\", \"beauty\", \"beings\", \"commence\", \"confessed\", \"confident\", \"confusion\", \"considered\", \"constrained\", \"courage\", \"crime\", \"decided\", \"declaration\", \"devices\", \"dim\", \"dreadfully\", \"dressed\", \"eleven\", \"engaging\", \"enormity\", \"entered\", \"affairs\", \"affection\", \"ancestors\", \"as\", \"assistance\", \"attention\", \"bear\", \"beaufort\", \"became\", \"begin\", \"birth\", \"bitterly\", \"business\", \"cannot\", \"character\", \"circumstances\", \"conduct\", \"could\", \"counsellors\", \"country\", \"credit\", \"daughter\", \"days\", \"debts\", \"decline\", \"deeply\", \"deplored\", \"disposition\", \"distinguished\", \"early\", \"adduced\", \"aggravation\", \"also\", \"although\", \"always\", \"appeared\", \"ascribed\", \"attend\", \"attest\", \"babe\", \"beauty\", \"beings\", \"commence\", \"confessed\", \"confident\", \"confusion\", \"considered\", \"constrained\", \"courage\", \"crime\", \"decided\", \"declaration\", \"devices\", \"dim\", \"dreadfully\", \"dressed\", \"eleven\", \"engaging\", \"enormity\", \"entered\", \"affairs\", \"affection\", \"ancestors\", \"as\", \"assistance\", \"attention\", \"bear\", \"beaufort\", \"became\", \"begin\", \"birth\", \"bitterly\", \"business\", \"cannot\", \"character\", \"circumstances\", \"conduct\", \"could\", \"counsellors\", \"country\", \"credit\", \"daughter\", \"days\", \"debts\", \"decline\", \"deeply\", \"deplored\", \"disposition\", \"distinguished\", \"early\", \"adduced\", \"aggravation\", \"also\", \"although\", \"always\", \"appeared\", \"ascribed\", \"attend\", \"attest\", \"babe\", \"beauty\", \"beings\", \"commence\", \"confessed\", \"confident\", \"confusion\", \"considered\", \"constrained\", \"courage\", \"crime\", \"decided\", \"declaration\", \"devices\", \"dim\", \"dreadfully\", \"dressed\", \"eleven\", \"engaging\", \"enormity\", \"entered\", \"affairs\", \"affection\", \"ancestors\", \"as\", \"assistance\", \"attention\", \"bear\", \"beaufort\", \"became\", \"begin\", \"birth\", \"bitterly\", \"business\", \"cannot\", \"character\", \"circumstances\", \"conduct\", \"could\", \"counsellors\", \"country\", \"credit\", \"daughter\", \"days\", \"debts\", \"decline\", \"deeply\", \"deplored\", \"disposition\", \"distinguished\", \"early\"], \"Freq\": [9.0, 7.0, 6.0, 3.0, 5.0, 2.0, 2.0, 2.0, 2.0, 3.0, 4.0, 3.0, 4.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 4.0, 3.0, 4.0, 2.0, 2.0, 2.0, 4.0, 2.0, 7.0, 3.4283982610822843, 2.592186058546692, 2.592186058546692, 2.592186058546692, 2.5921849204351526, 1.755969872620713, 1.755969872620713, 1.755969872620713, 1.755969872620713, 1.755969872620713, 1.755969872620713, 1.755969872620713, 1.755969872620713, 1.755969872620713, 1.755969872620713, 1.755969872620713, 1.755969872620713, 1.755969872620713, 1.755969872620713, 1.755969872620713, 1.755969872620713, 1.755969872620713, 1.755969872620713, 1.755969872620713, 1.755969872620713, 1.755969872620713, 1.755969872620713, 1.755969872620713, 1.755969872620713, 1.755969872620713, 23.497461758658797, 3.428398488704592, 2.592185603302076, 2.5921849204351526, 2.592186058546692, 2.5921849204351526, 2.592184237568229, 1.7559701002430206, 1.7559701002430206, 1.7559701002430206, 1.7559701002430206, 1.7559701002430206, 1.7559701002430206, 1.7559701002430206, 1.755969872620713, 1.755969872620713, 1.755969872620713, 1.755969872620713, 2.447931654316981, 2.447931654316981, 2.447931654316981, 1.6582566134913204, 1.6582566134913204, 1.6582566134913204, 1.6582566134913204, 1.6582566134913204, 1.6582566134913204, 1.6582566134913204, 1.6582566134913204, 1.6582566134913204, 1.6582566134913204, 1.6582566134913204, 1.6582566134913204, 1.6582566134913204, 0.8685731227081297, 0.8685731227081297, 0.8685731227081297, 0.8685731227081297, 0.8685731227081297, 0.8685731227081297, 0.8685731227081297, 0.8685731227081297, 0.8685731227081297, 0.8685731227081297, 0.8685731227081297, 0.8685731227081297, 0.8685731227081297, 0.8685731227081297, 7.185963839557795, 2.4479321513733066, 1.6582566134913204, 2.4479321513733066, 1.6582567791767622, 1.6582567791767622, 1.6582567791767622, 1.6582567791767622, 2.4479321513733066, 2.447932814115074, 2.4479321513733066, 1.6582572762330874, 1.6582567791767622, 7.185961188590727, 1.6582572762330874, 1.6582572762330874, 1.6582571105476456, 2.4315991123338954, 1.6471936403360221, 1.6471936403360221, 1.6471936403360221, 1.6471936403360221, 1.6471936403360221, 1.6471936403360221, 1.6471936403360221, 1.6471936403360221, 1.6471936403360221, 1.6471936403360221, 1.6471936403360221, 1.6471936403360221, 1.6471936403360221, 1.6471936403360221, 1.6471936403360221, 1.6471936403360221, 1.6471936403360221, 1.6471936403360221, 0.8627793373715288, 0.8627793373715288, 0.8627793373715288, 0.8627793373715288, 0.8627793373715288, 0.8627793373715288, 0.8627793373715288, 0.8627793373715288, 0.8627793373715288, 0.8627793373715288, 0.8627793373715288, 2.431599272896925, 4.784808784680279, 2.4315994334599544, 1.6471936403360221, 1.6471936403360221, 1.6471938008990517, 1.6471936403360221, 1.6471936403360221, 1.6471936403360221, 1.6471936403360221, 2.431599272896925, 2.4315994334599544, 11.060028476208847, 3.2160039420796513, 2.431598630644807, 1.6471941220251105, 1.6471938008990517, 1.6471938008990517, 1.6471936403360221, 1.6471936403360221, 1.6471936403360221, 1.6471936403360221, 1.6471936403360221, 1.6471936403360221, 2.001098432389405, 1.3555734136033009, 1.3555734136033009, 1.3555734136033009, 1.3555734136033009, 1.3555734136033009, 1.3555734136033009, 1.3555734136033009, 1.3555734136033009, 0.7100444180493759, 0.7100444180493759, 0.7100444180493759, 0.7100444180493759, 0.7100444180493759, 0.7100444180493759, 0.7100444180493759, 0.7100444180493759, 0.7100444180493759, 0.7100444180493759, 0.7100444180493759, 0.7100444180493759, 0.7100444180493759, 0.7100444180493759, 0.7100444180493759, 0.7100444180493759, 0.7100444180493759, 0.7100444180493759, 0.7100444180493759, 0.7100444180493759, 0.7100444180493759, 2.6466232921047967, 1.355574208956865, 1.3555740498861522, 1.3555740498861522, 1.3555740498861522, 3.29214656111306, 0.7100445771200887, 0.7100445771200887, 0.7100445771200887, 0.7100445771200887, 0.7100445771200887, 0.7100445771200887, 0.7100445771200887, 0.7100445771200887, 0.7100445771200887, 0.7100445771200887, 0.7100444975847323, 0.7100444975847323, 0.7100444975847323, 0.7100444975847323, 0.7100444975847323, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456, 0.0008494645306942456], \"Total\": [9.0, 7.0, 6.0, 3.0, 5.0, 2.0, 2.0, 2.0, 2.0, 3.0, 4.0, 3.0, 4.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 4.0, 3.0, 4.0, 2.0, 2.0, 2.0, 4.0, 2.0, 7.0, 3.6554547139411255, 2.819242511405533, 2.819242511405533, 2.819242511405533, 2.819241373293994, 1.9830263254795533, 1.9830263254795533, 1.9830263254795533, 1.9830263254795533, 1.9830263254795533, 1.9830263254795533, 1.9830263254795533, 1.9830263254795533, 1.9830263254795533, 1.9830263254795533, 1.9830263254795533, 1.9830263254795533, 1.9830263254795533, 1.9830263254795533, 1.9830263254795533, 1.9830263254795533, 1.9830263254795533, 1.9830263254795533, 1.9830263254795533, 1.9830263254795533, 1.9830263254795533, 1.9830263254795533, 1.9830263254795533, 1.9830263254795533, 1.9830263254795533, 45.040694771755604, 5.0905526204782365, 3.4647341423385076, 3.603580171493887, 3.60884843551363, 6.618036731316868, 7.525609257061773, 2.7726321458390752, 2.772632477209959, 2.767365351301755, 2.767365351301755, 2.76736543158327, 2.76736543158327, 2.76736543158327, 3.562315823213563, 3.55697121091178, 4.202463293035847, 3.412857289392394, 2.6796420438006177, 2.6796420438006177, 2.6796420438006177, 1.8899670029749562, 1.8899670029749562, 1.8899670029749562, 1.8899670029749562, 1.8899670029749562, 1.8899670029749562, 1.8899670029749562, 1.8899670029749562, 1.8899670029749562, 1.8899670029749562, 1.8899670029749562, 1.8899670029749562, 1.8899670029749562, 1.1002835121917656, 1.1002835121917656, 1.1002835121917656, 1.1002835121917656, 1.1002835121917656, 1.1002835121917656, 1.1002835121917656, 1.1002835121917656, 1.1002835121917656, 1.1002835121917656, 1.1002835121917656, 1.1002835121917656, 1.1002835121917656, 1.1002835121917656, 9.770832641640657, 4.161254366410972, 2.535459089152547, 4.248395963147388, 2.674305966860292, 2.6743061274233213, 2.6743061274233213, 2.6743061274233213, 4.893887969789621, 5.7300074271428, 6.618036731316868, 3.371579889609792, 3.3715802235073418, 45.040694771755604, 3.5104274894915473, 4.9403327997500455, 3.562315978836426, 2.6638363995292673, 1.8794309275313927, 1.8794309275313927, 1.8794309275313927, 1.8794309275313927, 1.8794309275313927, 1.8794309275313927, 1.8794309275313927, 1.8794309275313927, 1.8794309275313927, 1.8794309275313927, 1.8794309275313927, 1.8794309275313927, 1.8794309275313927, 1.8794309275313927, 1.8794309275313927, 1.8794309275313927, 1.8794309275313927, 1.8794309275313927, 1.0950166245668993, 1.0950166245668993, 1.0950166245668993, 1.0950166245668993, 1.0950166245668993, 1.0950166245668993, 1.0950166245668993, 1.0950166245668993, 1.0950166245668993, 1.0950166245668993, 1.0950166245668993, 3.453442318514952, 7.525609257061773, 4.098933902513805, 2.6690365202686075, 2.6690368516394907, 2.7155512600856992, 2.7155519531063237, 2.7155519531063237, 2.7155519531063237, 2.7155519531063237, 4.744463622315713, 4.790978667566545, 45.040694771755604, 7.345370390643929, 9.770832641640657, 3.5051571739489713, 3.505158120620171, 3.3610433915228484, 4.248395963147388, 4.893887969789621, 5.7300074271428, 4.9403327997500455, 3.551779520147692, 3.551779520147692, 2.2472236057585677, 1.6016985869724625, 1.6016985869724625, 1.6016985869724625, 1.6016985869724625, 1.6016985869724625, 1.6016985869724625, 1.6016985869724625, 1.6016985869724625, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 6.133848063901999, 2.437819042167111, 2.43781996430236, 4.744463622315713, 4.790978667566545, 45.040694771755604, 2.5301142217078727, 4.202463293035847, 3.3610433915228484, 1.792289922480526, 1.792289922480526, 1.792289922480526, 1.7922903208195644, 1.7922903208195644, 1.7922903208195644, 1.7922903208195644, 5.7300074271428, 2.5301144735434, 2.5818967034796425, 3.371579889609792, 4.9403327997500455, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 1.1002834293490444, 2.5301142217078727, 1.1002834293490444, 1.9364046825462822, 1.1002834293490444, 1.1002834293490444, 1.1002834293490444, 1.8899670029749562, 2.7726321458390752, 1.1002834293490444, 1.8899670029749562, 1.1002834293490444, 1.1002834293490444, 2.7207431759108887, 1.1002834293490444, 2.6796420438006177, 1.1002834293490444, 4.098933902513805, 1.1002834293490444, 4.248395963147388, 1.1002834293490444, 1.1002834293490444, 1.1002834293490444, 1.1002834293490444, 1.1002834293490444, 2.6796420438006177, 1.1002834293490444, 1.8899670029749562, 1.8899670029749562, 1.8846225486749963, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 1.1002834293490444, 2.5301142217078727, 1.1002834293490444, 1.9364046825462822, 1.1002834293490444, 1.1002834293490444, 1.1002834293490444, 1.8899670029749562, 2.7726321458390752, 1.1002834293490444, 1.8899670029749562, 1.1002834293490444, 1.1002834293490444, 2.7207431759108887, 1.1002834293490444, 2.6796420438006177, 1.1002834293490444, 4.098933902513805, 1.1002834293490444, 4.248395963147388, 1.1002834293490444, 1.1002834293490444, 1.1002834293490444, 1.1002834293490444, 1.1002834293490444, 2.6796420438006177, 1.1002834293490444, 1.8899670029749562, 1.8899670029749562, 1.8846225486749963, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 1.1002834293490444, 2.5301142217078727, 1.1002834293490444, 1.9364046825462822, 1.1002834293490444, 1.1002834293490444, 1.1002834293490444, 1.8899670029749562, 2.7726321458390752, 1.1002834293490444, 1.8899670029749562, 1.1002834293490444, 1.1002834293490444, 2.7207431759108887, 1.1002834293490444, 2.6796420438006177, 1.1002834293490444, 4.098933902513805, 1.1002834293490444, 4.248395963147388, 1.1002834293490444, 1.1002834293490444, 1.1002834293490444, 1.1002834293490444, 1.1002834293490444, 2.6796420438006177, 1.1002834293490444, 1.8899670029749562, 1.8899670029749562, 1.8846225486749963, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 1.1002834293490444, 2.5301142217078727, 1.1002834293490444, 1.9364046825462822, 1.1002834293490444, 1.1002834293490444, 1.1002834293490444, 1.8899670029749562, 2.7726321458390752, 1.1002834293490444, 1.8899670029749562, 1.1002834293490444, 1.1002834293490444, 2.7207431759108887, 1.1002834293490444, 2.6796420438006177, 1.1002834293490444, 4.098933902513805, 1.1002834293490444, 4.248395963147388, 1.1002834293490444, 1.1002834293490444, 1.1002834293490444, 1.1002834293490444, 1.1002834293490444, 2.6796420438006177, 1.1002834293490444, 1.8899670029749562, 1.8899670029749562, 1.8846225486749963, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 1.1002834293490444, 2.5301142217078727, 1.1002834293490444, 1.9364046825462822, 1.1002834293490444, 1.1002834293490444, 1.1002834293490444, 1.8899670029749562, 2.7726321458390752, 1.1002834293490444, 1.8899670029749562, 1.1002834293490444, 1.1002834293490444, 2.7207431759108887, 1.1002834293490444, 2.6796420438006177, 1.1002834293490444, 4.098933902513805, 1.1002834293490444, 4.248395963147388, 1.1002834293490444, 1.1002834293490444, 1.1002834293490444, 1.1002834293490444, 1.1002834293490444, 2.6796420438006177, 1.1002834293490444, 1.8899670029749562, 1.8899670029749562, 1.8846225486749963, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 0.9561695914185375, 1.1002834293490444, 2.5301142217078727, 1.1002834293490444, 1.9364046825462822, 1.1002834293490444, 1.1002834293490444, 1.1002834293490444, 1.8899670029749562, 2.7726321458390752, 1.1002834293490444, 1.8899670029749562, 1.1002834293490444, 1.1002834293490444, 2.7207431759108887, 1.1002834293490444, 2.6796420438006177, 1.1002834293490444, 4.098933902513805, 1.1002834293490444, 4.248395963147388, 1.1002834293490444, 1.1002834293490444, 1.1002834293490444, 1.1002834293490444, 1.1002834293490444, 2.6796420438006177, 1.1002834293490444, 1.8899670029749562, 1.8899670029749562, 1.8846225486749963], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.9599, -5.2395, -5.2395, -5.2395, -5.2395, -5.629, -5.629, -5.629, -5.629, -5.629, -5.629, -5.629, -5.629, -5.629, -5.629, -5.629, -5.629, -5.629, -5.629, -5.629, -5.629, -5.629, -5.629, -5.629, -5.629, -5.629, -5.629, -5.629, -5.629, -5.629, -3.0351, -4.9599, -5.2395, -5.2395, -5.2395, -5.2395, -5.2395, -5.629, -5.629, -5.629, -5.629, -5.629, -5.629, -5.629, -5.629, -5.629, -5.629, -5.629, -4.9791, -4.9791, -4.9791, -5.3686, -5.3686, -5.3686, -5.3686, -5.3686, -5.3686, -5.3686, -5.3686, -5.3686, -5.3686, -5.3686, -5.3686, -5.3686, -6.0153, -6.0153, -6.0153, -6.0153, -6.0153, -6.0153, -6.0153, -6.0153, -6.0153, -6.0153, -6.0153, -6.0153, -6.0153, -6.0153, -3.9023, -4.9791, -5.3686, -4.9791, -5.3686, -5.3686, -5.3686, -5.3686, -4.9791, -4.9791, -4.9791, -5.3686, -5.3686, -3.9023, -5.3686, -5.3686, -5.3686, -4.9544, -5.3439, -5.3439, -5.3439, -5.3439, -5.3439, -5.3439, -5.3439, -5.3439, -5.3439, -5.3439, -5.3439, -5.3439, -5.3439, -5.3439, -5.3439, -5.3439, -5.3439, -5.3439, -5.9906, -5.9906, -5.9906, -5.9906, -5.9906, -5.9906, -5.9906, -5.9906, -5.9906, -5.9906, -5.9906, -4.9544, -4.2775, -4.9544, -5.3439, -5.3439, -5.3439, -5.3439, -5.3439, -5.3439, -5.3439, -4.9544, -4.9544, -3.4396, -4.6748, -4.9544, -5.3439, -5.3439, -5.3439, -5.3439, -5.3439, -5.3439, -5.3439, -5.3439, -5.3439, -4.4468, -4.8363, -4.8363, -4.8363, -4.8363, -4.8363, -4.8363, -4.8363, -4.8363, -5.4829, -5.4829, -5.4829, -5.4829, -5.4829, -5.4829, -5.4829, -5.4829, -5.4829, -5.4829, -5.4829, -5.4829, -5.4829, -5.4829, -5.4829, -5.4829, -5.4829, -5.4829, -5.4829, -5.4829, -5.4829, -4.1672, -4.8363, -4.8363, -4.8363, -4.8363, -3.949, -5.4829, -5.4829, -5.4829, -5.4829, -5.4829, -5.4829, -5.4829, -5.4829, -5.4829, -5.4829, -5.4829, -5.4829, -5.4829, -5.4829, -5.4829, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416, -6.8416], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.9628, 0.943, 0.943, 0.943, 0.943, 0.9053, 0.9053, 0.9053, 0.9053, 0.9053, 0.9053, 0.9053, 0.9053, 0.9053, 0.9053, 0.9053, 0.9053, 0.9053, 0.9053, 0.9053, 0.9053, 0.9053, 0.9053, 0.9053, 0.9053, 0.9053, 0.9053, 0.9053, 0.9053, 0.9053, 0.3763, 0.6316, 0.7368, 0.6975, 0.696, 0.0896, -0.0389, 0.5701, 0.5701, 0.5721, 0.5721, 0.5721, 0.5721, 0.5721, 0.3195, 0.321, 0.1543, 0.3624, 1.2541, 1.2541, 1.2541, 1.2137, 1.2137, 1.2137, 1.2137, 1.2137, 1.2137, 1.2137, 1.2137, 1.2137, 1.2137, 1.2137, 1.2137, 1.2137, 1.1081, 1.1081, 1.1081, 1.1081, 1.1081, 1.1081, 1.1081, 1.1081, 1.1081, 1.1081, 1.1081, 1.1081, 1.1081, 1.1081, 1.0372, 0.8139, 0.9199, 0.7932, 0.8666, 0.8666, 0.8666, 0.8666, 0.6518, 0.494, 0.35, 0.6349, 0.6349, -0.4909, 0.5946, 0.2529, 0.5799, 1.2847, 1.244, 1.244, 1.244, 1.244, 1.244, 1.244, 1.244, 1.244, 1.244, 1.244, 1.244, 1.244, 1.244, 1.244, 1.244, 1.244, 1.244, 1.244, 1.1376, 1.1376, 1.1376, 1.1376, 1.1376, 1.1376, 1.1376, 1.1376, 1.1376, 1.1376, 1.1376, 1.0251, 0.9231, 0.8537, 0.8933, 0.8933, 0.876, 0.876, 0.876, 0.876, 0.876, 0.7075, 0.6977, -0.0283, 0.55, -0.0149, 0.6208, 0.6208, 0.6627, 0.4285, 0.287, 0.1293, 0.2776, 0.6076, 0.6076, 1.9624, 1.9116, 1.9116, 1.9116, 1.9116, 1.9116, 1.9116, 1.9116, 1.9116, 1.7808, 1.7808, 1.7808, 1.7808, 1.7808, 1.7808, 1.7808, 1.7808, 1.7808, 1.7808, 1.7808, 1.7808, 1.7808, 1.7808, 1.7808, 1.7808, 1.7808, 1.7808, 1.7808, 1.7808, 1.7808, 1.2379, 1.4915, 1.4915, 0.8257, 0.8159, -0.5376, 0.8077, 0.3003, 0.5237, 1.1525, 1.1525, 1.1525, 1.1525, 1.1525, 1.1525, 1.1525, -0.0097, 0.8077, 0.7875, 0.5206, 0.1386, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.2817, -0.551, 0.2817, -0.2835, 0.2817, 0.2817, 0.2817, -0.2593, -0.6425, 0.2817, -0.2593, 0.2817, 0.2817, -0.6236, 0.2817, -0.6084, 0.2817, -1.0334, 0.2817, -1.0692, 0.2817, 0.2817, 0.2817, 0.2817, 0.2817, -0.6084, 0.2817, -0.2593, -0.2593, -0.2564, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.2817, -0.551, 0.2817, -0.2835, 0.2817, 0.2817, 0.2817, -0.2593, -0.6425, 0.2817, -0.2593, 0.2817, 0.2817, -0.6236, 0.2817, -0.6084, 0.2817, -1.0334, 0.2817, -1.0692, 0.2817, 0.2817, 0.2817, 0.2817, 0.2817, -0.6084, 0.2817, -0.2593, -0.2593, -0.2564, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.2817, -0.551, 0.2817, -0.2835, 0.2817, 0.2817, 0.2817, -0.2593, -0.6425, 0.2817, -0.2593, 0.2817, 0.2817, -0.6236, 0.2817, -0.6084, 0.2817, -1.0334, 0.2817, -1.0692, 0.2817, 0.2817, 0.2817, 0.2817, 0.2817, -0.6084, 0.2817, -0.2593, -0.2593, -0.2564, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.2817, -0.551, 0.2817, -0.2835, 0.2817, 0.2817, 0.2817, -0.2593, -0.6425, 0.2817, -0.2593, 0.2817, 0.2817, -0.6236, 0.2817, -0.6084, 0.2817, -1.0334, 0.2817, -1.0692, 0.2817, 0.2817, 0.2817, 0.2817, 0.2817, -0.6084, 0.2817, -0.2593, -0.2593, -0.2564, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.2817, -0.551, 0.2817, -0.2835, 0.2817, 0.2817, 0.2817, -0.2593, -0.6425, 0.2817, -0.2593, 0.2817, 0.2817, -0.6236, 0.2817, -0.6084, 0.2817, -1.0334, 0.2817, -1.0692, 0.2817, 0.2817, 0.2817, 0.2817, 0.2817, -0.6084, 0.2817, -0.2593, -0.2593, -0.2564, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.4221, 0.2817, -0.551, 0.2817, -0.2835, 0.2817, 0.2817, 0.2817, -0.2593, -0.6425, 0.2817, -0.2593, 0.2817, 0.2817, -0.6236, 0.2817, -0.6084, 0.2817, -1.0334, 0.2817, -1.0692, 0.2817, 0.2817, 0.2817, 0.2817, 0.2817, -0.6084, 0.2817, -0.2593, -0.2593, -0.2564]}, \"token.table\": {\"Topic\": [1, 4, 1, 4, 2, 4, 2, 2, 2, 2, 3, 4, 3, 2, 4, 2, 3, 3, 3, 1, 2, 1, 4, 4, 4, 2, 2, 2, 3, 4, 2, 4, 1, 2, 3, 2, 1, 2, 4, 2, 2, 3, 4, 3, 1, 3, 3, 2, 3, 4, 2, 4, 2, 2, 4, 1, 2, 1, 3, 3, 3, 2, 2, 4, 2, 2, 2, 1, 2, 2, 2, 3, 4, 1, 2, 3, 1, 3, 4, 2, 2, 3, 2, 1, 4, 4, 2, 4, 4, 4, 4, 4, 2, 3, 4, 2, 2, 3, 4, 4, 2, 4, 2, 1, 3, 2, 3, 2, 4, 4, 2, 2, 2, 4, 4, 1, 4, 1, 2, 2, 4, 4, 2, 3, 4, 2, 3, 1, 4, 4, 2, 3, 4, 1, 3, 1, 2, 3, 1, 4, 2, 4, 2, 3, 4, 3, 1, 3, 1, 2, 2, 3, 1, 2, 3, 1, 1, 1, 3, 4, 2, 3, 3, 1, 2, 3, 1, 2, 3, 4, 3, 3, 1, 1, 3, 4, 1, 1, 2, 4, 3, 4, 1, 4, 1, 1, 1, 2, 3, 4, 1, 1, 3, 3, 3, 2, 1, 1, 3, 1, 2, 4, 2, 3, 1, 1, 2, 3, 4, 2, 1, 4, 1, 3, 1, 2, 3, 4, 3, 1, 1, 3, 1, 3, 4, 1, 3, 1, 2, 3, 1, 2, 3, 4, 2, 3, 1, 2, 4, 1, 3, 2, 1, 1, 2, 1, 4, 1, 4, 3, 1, 3, 1, 1, 4, 1, 1, 4, 1, 2, 3, 4, 3, 1, 2, 1, 1, 3, 4, 1, 2, 4, 2, 2, 3, 1, 3, 1, 2, 3, 2, 1, 3, 1, 4, 3, 2, 1, 3, 3, 1, 2, 3, 4, 1, 1, 2, 4, 1, 1, 3, 4, 1, 2, 3, 1, 3, 4, 1, 2, 1, 3, 4, 1, 3], \"Freq\": [0.410202564029856, 0.410202564029856, 0.5579453219067365, 0.5579453219067365, 0.9088566618689026, 1.0458395759233854, 0.9088566618689026, 0.9088566618689026, 0.908856730298688, 0.3952390731691876, 0.3952390731691876, 0.3952390731691876, 0.9132281442718002, 0.9088566618689026, 1.0458395759233854, 0.9088566618689026, 0.9132281442718002, 0.9132281442718002, 0.9132281442718002, 1.008559480175505, 0.9088566618689026, 1.008559480175505, 1.0458395759233854, 1.0458395759233854, 1.0458395759233854, 1.0582195333843625, 0.9088566618689026, 0.908856730298688, 0.9132281442718002, 0.6243371931108488, 0.9088566618689026, 1.0458395759233854, 0.7213361368444178, 0.3606680684222089, 0.9132281442718002, 0.9088566618689026, 0.5164209780184205, 0.5164209780184205, 1.0458395759233854, 0.908856730298688, 0.9088566618689026, 0.9132281442718002, 1.0458395759233854, 0.9132281442718002, 0.3682492610226435, 0.736498522045287, 1.064151904016485, 0.908856730298688, 0.9132281442718002, 1.0458395759233854, 0.9088566618689026, 1.0458395759233854, 0.908856730298688, 1.0582195333843625, 1.0458395759233854, 0.7213362230548419, 0.36066811152742095, 0.3682492610226435, 0.736498522045287, 0.9132281442718002, 0.9132281442718002, 0.9088566618689026, 0.908856730298688, 1.0458395759233854, 0.9088566618689026, 1.0582195333843625, 0.908856730298688, 1.008559480175505, 0.9088566618689026, 0.908856730298688, 0.39523903382897535, 0.39523903382897535, 0.39523903382897535, 0.3675466353656133, 0.3675466353656133, 0.3675466353656133, 0.3682493550014745, 0.736498710002949, 0.6243371931108488, 0.908856730298688, 1.0582195333843625, 1.064151904016485, 0.7463683459613655, 1.008559480175505, 1.0458395759233854, 0.6243371931108488, 0.908856730298688, 1.0458395759233854, 1.0458395759233854, 1.0458395759233854, 1.0458395759233854, 1.0458395759233854, 0.2439658759529441, 0.4879317519058882, 0.2439658759529441, 0.908856730298688, 0.47076591196982426, 0.47076591196982426, 1.0458395759233854, 0.6243371931108488, 0.908856730298688, 1.0458395759233854, 0.908856730298688, 0.5630980157002637, 0.5630980157002637, 0.908856730298688, 1.064151904016485, 0.908856730298688, 1.0458395759233854, 1.0458395759233854, 0.908856730298688, 0.7463683459613655, 0.908856730298688, 1.0458395759233854, 1.0458395759233854, 0.5579453219067365, 0.5579453219067365, 1.008559480175505, 1.0582195333843625, 1.0582195333843625, 1.0458395759233854, 1.0458395759233854, 0.5306102278692678, 0.5306102278692678, 1.0458395759233854, 0.28956615103680655, 0.5791323020736131, 1.008559480175505, 1.0458395759233854, 1.0458395759233854, 0.3746669887250665, 0.749333977450133, 1.0458395759233854, 0.5630980698310233, 0.5630980698310233, 0.5622761280340326, 0.2811380640170163, 0.2811380640170163, 0.5579454459108947, 0.5579454459108947, 0.7888117810918736, 0.3944058905459368, 0.4086730248722829, 0.4086730248722829, 0.20433651243614145, 1.064151904016485, 0.5630980157002637, 0.5630980157002637, 0.8312901064167368, 0.2770967021389123, 1.0582195333843625, 1.064151904016485, 0.28486559058504884, 0.5697311811700977, 0.28486559058504884, 1.008559480175505, 0.8206913324787308, 0.2975266557171438, 0.5950533114342876, 0.2975266557171438, 0.7164179611641167, 0.20469084604689047, 1.064151904016485, 1.0641160520763813, 0.3746670352413768, 0.7493340704827536, 0.5106493165026171, 0.15541500937036173, 0.2442235861534256, 0.06660643258729788, 1.064151904016485, 1.064151904016485, 1.0641156224990203, 0.3682492610226435, 0.736498522045287, 0.6243371931108488, 1.008559480175505, 0.5893269795368823, 0.19644232651229407, 0.19644232651229407, 1.064151904016485, 0.8899870911265568, 0.5579453219067365, 0.5579453219067365, 1.008559480175505, 1.008559480175505, 0.1745198435979407, 0.3490396871958814, 0.3490396871958814, 0.1745198435979407, 1.008559480175505, 0.5630980698310233, 0.5630980698310233, 1.064151904016485, 1.064151904016485, 1.0582195333843625, 1.0641156224990203, 0.7227090557664205, 0.36135452788321026, 0.38731216421334447, 0.38731216421334447, 0.38731216421334447, 0.7478575393786308, 0.3739287696893154, 1.008559480175505, 0.47591135496991005, 0.23795567748495502, 0.23795567748495502, 0.23795567748495502, 1.0582195333843625, 0.5579454459108947, 0.5579454459108947, 1.008559480175505, 1.064151904016485, 0.2722803471622717, 0.2722803471622717, 0.4084205207434075, 0.13614017358113584, 1.064151904016485, 1.008559480175505, 1.008559480175505, 1.064151904016485, 0.3682492610226435, 0.736498522045287, 0.6243371931108488, 0.7227090348005671, 0.36135451740028357, 0.285293834283022, 0.285293834283022, 0.570587668566044, 0.45330664089606143, 0.30220442726404095, 0.15110221363202048, 0.15110221363202048, 0.7478575842793539, 0.37392879213967695, 0.29659682810683163, 0.5931936562136633, 0.29659682810683163, 1.008559480175505, 1.064151904016485, 1.0582195333843625, 1.008559480175505, 1.008559480175505, 1.0582195333843625, 1.0641156224990203, 0.6243371931108488, 0.5579453219067365, 0.5579453219067365, 1.064151904016485, 0.832505413291896, 0.277501804430632, 1.008559480175505, 0.5579454459108947, 0.5579454459108947, 1.008559480175505, 0.4102027191940568, 0.4102027191940568, 1.008559480175505, 0.2107719817465715, 0.421543963493143, 0.2107719817465715, 1.064151904016485, 0.5614325096038416, 0.5614325096038416, 1.008559480175505, 0.7227090348005671, 0.36135451740028357, 0.6243371931108488, 0.2965968574796946, 0.5931937149593892, 0.2965968574796946, 0.7463683459613655, 0.7478575393786308, 0.3739287696893154, 0.5630980157002637, 0.5630980157002637, 0.2852939113350465, 0.2852939113350465, 0.570587822670093, 1.0582195333843625, 0.7227090348005671, 0.36135451740028357, 0.8658673008530353, 0.2886224336176785, 1.064151904016485, 1.0582195333843625, 0.7227090557664205, 0.36135452788321026, 0.7507968583781739, 0.20241551339427877, 0.40483102678855754, 0.40483102678855754, 0.20241551339427877, 1.008559480175505, 0.2403121539677679, 0.4806243079355358, 0.2403121539677679, 1.008559480175505, 0.586019229756914, 0.293009614878457, 0.293009614878457, 1.008559480175505, 0.7478575393786308, 0.3739287696893154, 0.32605959247182853, 0.32605959247182853, 0.48908938870774277, 0.5614325341305088, 0.5614325341305088, 0.20872562150395138, 0.41745124300790276, 0.20872562150395138, 0.3986388207951274, 0.6643980346585456], \"Term\": [\"a\", \"a\", \"absent\", \"absent\", \"act\", \"adduced\", \"adventure\", \"aerial\", \"affairs\", \"affection\", \"affection\", \"affection\", \"age\", \"ages\", \"aggravation\", \"akin\", \"alarming\", \"alas\", \"all\", \"almost\", \"alpine\", \"already\", \"also\", \"although\", \"always\", \"among\", \"ample\", \"ancestors\", \"another\", \"appearance\", \"appearances\", \"appeared\", \"ardour\", \"ardour\", \"arguments\", \"arthur\", \"as\", \"as\", \"ascribed\", \"assistance\", \"attach\", \"attained\", \"attend\", \"attendants\", \"attended\", \"attended\", \"attending\", \"attention\", \"attentions\", \"attest\", \"avoid\", \"babe\", \"bear\", \"beaufort\", \"beauty\", \"became\", \"became\", \"become\", \"become\", \"been\", \"befitting\", \"began\", \"begin\", \"beings\", \"belrive\", \"birth\", \"bitterly\", \"black\", \"blood\", \"business\", \"calm\", \"calm\", \"calm\", \"cannot\", \"cannot\", \"cannot\", \"care\", \"care\", \"cause\", \"character\", \"characters\", \"children\", \"circumstances\", \"closely\", \"commence\", \"committed\", \"conduct\", \"confessed\", \"confident\", \"confusion\", \"considered\", \"constrained\", \"could\", \"could\", \"could\", \"counsellors\", \"country\", \"country\", \"courage\", \"court\", \"credit\", \"crime\", \"daughter\", \"day\", \"day\", \"days\", \"dear\", \"debts\", \"decided\", \"declaration\", \"decline\", \"deeply\", \"deplored\", \"devices\", \"dim\", \"discovered\", \"discovered\", \"discoveries\", \"disposition\", \"distinguished\", \"dreadfully\", \"dressed\", \"early\", \"early\", \"eleven\", \"elizabeth\", \"elizabeth\", \"engaged\", \"engaging\", \"enormity\", \"enter\", \"enter\", \"entered\", \"ernest\", \"ernest\", \"even\", \"even\", \"even\", \"eyes\", \"eyes\", \"family\", \"family\", \"father\", \"father\", \"father\", \"fever\", \"first\", \"first\", \"found\", \"found\", \"friendship\", \"future\", \"geneva\", \"geneva\", \"geneva\", \"gone\", \"great\", \"happy\", \"happy\", \"happy\", \"he\", \"he\", \"her\", \"his\", \"hope\", \"hope\", \"i\", \"i\", \"i\", \"i\", \"ill\", \"illness\", \"in\", \"ingolstadt\", \"ingolstadt\", \"innocence\", \"instruments\", \"it\", \"it\", \"it\", \"journey\", \"justine\", \"kindness\", \"kindness\", \"know\", \"krempe\", \"life\", \"life\", \"life\", \"life\", \"light\", \"long\", \"long\", \"looks\", \"love\", \"loved\", \"m\", \"made\", \"made\", \"make\", \"make\", \"make\", \"many\", \"many\", \"may\", \"me\", \"me\", \"me\", \"me\", \"merchant\", \"might\", \"might\", \"morning\", \"mother\", \"my\", \"my\", \"my\", \"my\", \"myself\", \"natural\", \"nearly\", \"necessary\", \"never\", \"never\", \"obliterated\", \"often\", \"often\", \"on\", \"on\", \"on\", \"one\", \"one\", \"one\", \"one\", \"parents\", \"parents\", \"passed\", \"passed\", \"passed\", \"philosophy\", \"pleased\", \"poverty\", \"proficiency\", \"progress\", \"public\", \"pursuit\", \"quickly\", \"rendered\", \"rendered\", \"resolved\", \"return\", \"return\", \"returning\", \"saw\", \"saw\", \"science\", \"seemed\", \"seemed\", \"sense\", \"she\", \"she\", \"she\", \"sickbed\", \"son\", \"son\", \"sought\", \"study\", \"study\", \"suffered\", \"the\", \"the\", \"the\", \"them\", \"therefore\", \"therefore\", \"thought\", \"thought\", \"time\", \"time\", \"time\", \"together\", \"true\", \"true\", \"two\", \"two\", \"uncle\", \"united\", \"university\", \"university\", \"upon\", \"us\", \"us\", \"us\", \"us\", \"waldman\", \"we\", \"we\", \"we\", \"went\", \"when\", \"when\", \"when\", \"whilst\", \"world\", \"world\", \"would\", \"would\", \"would\", \"years\", \"years\", \"yet\", \"yet\", \"yet\", \"you\", \"you\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [6, 7, 3, 4, 1, 2, 5, 8, 9, 10]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el187619023730194247185433141\", ldavis_el187619023730194247185433141_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el187619023730194247185433141\", ldavis_el187619023730194247185433141_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el187619023730194247185433141\", ldavis_el187619023730194247185433141_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "5     -0.120267  0.119835       1        1  35.810638\n",
       "6      0.121895 -0.138220       2        1  26.066432\n",
       "2      0.107011  0.148356       3        1  25.260550\n",
       "3     -0.131795 -0.105354       4        1  12.512886\n",
       "0      0.003855 -0.004103       5        1   0.058249\n",
       "1      0.003861 -0.004102       6        1   0.058249\n",
       "4      0.003865 -0.004102       7        1   0.058249\n",
       "7      0.003854 -0.004104       8        1   0.058249\n",
       "8      0.003862 -0.004102       9        1   0.058249\n",
       "9      0.003859 -0.004103      10        1   0.058249, topic_info=              Term      Freq     Total Category  logprob  loglift\n",
       "44              he  9.000000  9.000000  Default  30.0000  30.0000\n",
       "415            you  7.000000  7.000000  Default  29.0000  29.0000\n",
       "754          would  6.000000  6.000000  Default  28.0000  28.0000\n",
       "475          great  3.000000  3.000000  Default  27.0000  27.0000\n",
       "209             it  5.000000  5.000000  Default  26.0000  26.0000\n",
       "..             ...       ...       ...      ...      ...      ...\n",
       "25          deeply  0.000849  2.679642  Topic10  -6.8416  -0.6084\n",
       "26        deplored  0.000849  1.100283  Topic10  -6.8416   0.2817\n",
       "27     disposition  0.000849  1.889967  Topic10  -6.8416  -0.2593\n",
       "28   distinguished  0.000849  1.889967  Topic10  -6.8416  -0.2593\n",
       "29           early  0.000849  1.884623  Topic10  -6.8416  -0.2564\n",
       "\n",
       "[590 rows x 6 columns], token_table=      Topic      Freq    Term\n",
       "term                         \n",
       "416       1  0.410203       a\n",
       "416       4  0.410203       a\n",
       "761       1  0.557945  absent\n",
       "761       4  0.557945  absent\n",
       "118       2  0.908857     act\n",
       "...     ...       ...     ...\n",
       "577       1  0.208726     yet\n",
       "577       3  0.417451     yet\n",
       "577       4  0.208726     yet\n",
       "415       1  0.398639     you\n",
       "415       3  0.664398     you\n",
       "\n",
       "[315 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[6, 7, 3, 4, 1, 2, 5, 8, 9, 10])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim_models.prepare(lda_model, whole_file, id2word,  mds='mmds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
